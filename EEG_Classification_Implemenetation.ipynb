{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1322531c",
   "metadata": {},
   "source": [
    "# Classification of EEG Signals\n",
    "\n",
    "This notebook is a guide to classification of EEG Signals obtained from the [Keirn EEG Database](https://www.cs.colostate.edu/eeg/main/data/1989_Keirn_and_Aunon). This implementation is an attempt at reproducing the classification described by _Charles W. Anderson and Zlatko Sijerˇci ́c_ in this [paper](https://www.cs.colostate.edu/~anderson/wp/pubs/anderson-sijercic-96.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import math\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.regression.linear_model import burg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_affirm(to_print):\n",
    "    \"\"\"\n",
    "    Helper method for pretty printing important statements \n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    to_print : any\n",
    "        the message to print\n",
    "    \"\"\"\n",
    "    print('\\u279C ' + str(to_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mne_info(n_channels, s_freq, ch_names, ch_types):\n",
    "    \"\"\"\n",
    "    Method to create a global mne info object for the loaded eeg data\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of channels used for collecting data\n",
    "    s_freq\n",
    "        The sampling frequency used for data collection\n",
    "    ch_names : array\n",
    "        An array of strings containing channel names\n",
    "    ch_types : array\n",
    "        An array of strings containing the channel types for each channel in ch_names\n",
    "    \"\"\"\n",
    "    \n",
    "    info = mne.create_info(ch_names, ch_types=ch_types, sfreq=s_freq)\n",
    "    info.set_montage('standard_1020')\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_file_name):\n",
    "    \"\"\"\n",
    "    Method to load the eeg dataset\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file_name \n",
    "        The name of the file containing eeg data\n",
    "    \"\"\"\n",
    "    \n",
    "    mat = scipy.io.loadmat(input_file_name)\n",
    "    return mat['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_eeg_data(data_arr, info, description):\n",
    "    \"\"\"\n",
    "    Method to segment eeg data\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_arr \n",
    "        The data array from containing eeg data\n",
    "    info : info object from mne containing meta data\n",
    "        Info object from mne containing meta data\n",
    "    description : str\n",
    "        The description to give to each segment\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dict = {\n",
    "        'subject': [],\n",
    "        'trial': [],\n",
    "        'task': []\n",
    "    }\n",
    "    \n",
    "    coeff_arr = []\n",
    "    onset_arr = np.arange(0, 10, 0.25)\n",
    "    duration_arr = [0.5] * len(onset_arr)\n",
    "    description_arr =  [description] * 40\n",
    "    my_annot = mne.Annotations(\n",
    "                onset=onset_arr, \n",
    "                duration=duration_arr, \n",
    "                description=description_arr\n",
    "            )\n",
    "    mne.set_log_level('error')\n",
    "    for obs in data_arr:\n",
    "        raw = mne.io.RawArray(obs[0][3], info)\n",
    "        raw.set_annotations(my_annot)\n",
    "        burgs_values = gen_autoregression_coeff(obs[0][3], raw.annotations, raw)\n",
    "        for burg in burgs_values:\n",
    "            for coeff_arr in burg:\n",
    "                obs_index = 1\n",
    "                data_dict['subject'].append(obs[0][0][-1])\n",
    "                data_dict['task'].append(obs[0][1][0])\n",
    "                data_dict['trial'].append((obs[0][2][0][-2] + obs[0][2][0][-1]).strip())\n",
    "                for coeff in coeff_arr[0]:\n",
    "                    if obs_index in data_dict.keys():\n",
    "                        data_dict[obs_index].append(coeff)\n",
    "                    else:\n",
    "                        data_dict[obs_index] = [coeff]\n",
    "                    obs_index += 1\n",
    "        coeff_arr.append(burgs_values)\n",
    "    export_to_csv(data_dict, 'coefficients')\n",
    "    print_affirm('.csv file for coefficients created and saved')\n",
    "    print_affirm('EEG segmentation done successfully')\n",
    "    return coeff_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdff8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_autoregression_coeff(data_arr, annotations, annt_raw):\n",
    "    \"\"\"\n",
    "    Method to generate Burg's coefficients \n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_arr \n",
    "        The data array from which to extract sub arrays\n",
    "    annotations : array\n",
    "        Array containing annotations of the eeg signals\n",
    "    annt_raw : Raw object\n",
    "        A Raw array object from MNE containing the annotated eeg data \n",
    "    \"\"\"\n",
    "    \n",
    "    burgs_values = []\n",
    "    mne.set_log_level('error')\n",
    "    for annt in annotations:\n",
    "        start = math.floor(annt['onset']*250)\n",
    "        duration = math.floor(annt['duration']*250)\n",
    "        end = start + duration\n",
    "        obs_values = []\n",
    "        for segment in data_arr[:6]:\n",
    "            # for each segment find the burg's coefficients\n",
    "            rho_sigma = list(burg(segment[start:end], order=6))\n",
    "            obs_values.append(rho_sigma)\n",
    "        burgs_values.append(obs_values)\n",
    "    return burgs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4568018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_eye_blinks(data_arr): \n",
    "    \"\"\"\n",
    "    Method to remove eye blinks from the given eeg data\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_arr \n",
    "        The data array from which to remove eye blink data\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = data_arr\n",
    "    for obs in temp:\n",
    "        for eeg in obs[0][3]:\n",
    "            win_coeffs = signal.firwin(numtaps=10, cutoff=30, window='hamming', pass_zero=True, scale=True, fs=250)\n",
    "            lfilt_result = signal.lfilter(win_coeffs, [1.0], eeg)\n",
    "            # find peaks in the eeg signal that can be discarded\n",
    "            b_peaks = signal.find_peaks(lfilt_result, height=28)\n",
    "            # find peak widths\n",
    "            b_widths = signal.peak_widths(lfilt_result, b_peaks[0], rel_height=0.6, prominence_data=None, wlen=None)\n",
    "            left_points = b_widths[2]\n",
    "            right_points = b_widths[3]\n",
    "            eeg_mean = np.mean(eeg)\n",
    "            if len(left_points) > 0 and len(right_points) > 0:\n",
    "                # optimise the values of the peaks to fit with general data by replacing it with mean value\n",
    "                for idx in range(0, len(left_points)):\n",
    "                    width_arr = np.arange(int(left_points[idx]), int(right_points[idx]+1), 1)\n",
    "                    for point in width_arr:\n",
    "                        eeg[point] = (eeg[point] - lfilt_result[point])  + eeg_mean\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e7e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_mat(data_arr, output_file_name):\n",
    "    \"\"\"\n",
    "    Method to create a .mat file from a data array in appropriate dimensions and shape\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_arr \n",
    "        The data array from which to create the .mat file\n",
    "    output_file_name : str\n",
    "        The file name of the output file\n",
    "    \"\"\"\n",
    "    \n",
    "    new_mat_file = {\n",
    "        'data' : [data_arr]\n",
    "    }\n",
    "    scipy.io.savemat(output_file_name, new_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e575a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(data, output_file_name):\n",
    "    \"\"\"\n",
    "    Method to export data to csv format\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Dictionary containing key-value pairs, with the values being equal length arrays\n",
    "    output_file_name : str\n",
    "        The file name of the output file\n",
    "    \"\"\"\n",
    "    # create a dataframe\n",
    "    dataframe = pd.DataFrame(data)\n",
    "#     print(data.keys())\n",
    "#     for key in data.keys():\n",
    "#         print(len(data[key]))\n",
    "    # convert the dataframe to csv\n",
    "    dataframe.to_csv(output_file_name + '.csv', encoding='utf-8')\n",
    "    print(output_file_name + '.csv created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20add46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_dataset(input_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    Method to create the csv dataset from the original .mat file\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file_name : str\n",
    "        The input file name to be read\n",
    "    output_file_name : str\n",
    "        The file name of the output file\n",
    "    \"\"\"\n",
    "    \n",
    "    mat = scipy.io.loadmat(input_file_name)\n",
    "    data = mat['data'][0]\n",
    "    \n",
    "    subject_data = {\n",
    "        'subject 1': [],\n",
    "        'subject 2': [],\n",
    "        'subject 3': [],\n",
    "        'subject 4': [],\n",
    "        'subject 5': [],\n",
    "        'subject 6': [],\n",
    "        'subject 7': [],\n",
    "    }\n",
    "    # create subject wise data lists\n",
    "    for obs in data:\n",
    "        if obs[0][0][0] in subject_data.keys():\n",
    "            subject_data[obs[0][0][0]].append(obs)\n",
    "        else:\n",
    "            subject_data[obs[0][0][0]] = [obs]\n",
    "        \n",
    "    # index [0][0][3] of every subject  in subject_data contains 7 lists\n",
    "    # index [0][0][3][0] of every subject in subject_data contains 2500 values\n",
    "    for key in subject_data.keys():\n",
    "        print(key + ' has ' + str(len(subject_data[key])) + ' observations')\n",
    "        \n",
    "    # initialise the data dictionary with required keys\n",
    "    data = {\n",
    "        'subject' : [],\n",
    "        'task' : [],\n",
    "        'trial' : []\n",
    "    }\n",
    "    # for every subject, create the data row\n",
    "    for sub in subject_data.keys():\n",
    "        for obs in subject_data[sub]:\n",
    "            # loop through the 7 channels\n",
    "            for signal_values in obs[0][3]:\n",
    "                data['subject'].append(sub[-1])\n",
    "                data['task'].append(obs[0][1][0])\n",
    "                data['trial'].append((obs[0][2][0][-2] + obs[0][2][0][-1]).strip())\n",
    "                # initialise the index for the 2500 observations\n",
    "                obs_index = 1\n",
    "                # run through the 2500 values for every channel\n",
    "                for val in signal_values:\n",
    "                    if obs_index in data.keys():\n",
    "                        data[obs_index].append(val)\n",
    "                    else:\n",
    "                        data[obs_index] = [val]\n",
    "                    obs_index += 1\n",
    "    \n",
    "    # print the number of rows in the dataset \n",
    "    print('The dataset is ' + str(len(data['subject'])) + ' rows X ' + str(len(data.keys())) + ' columns')\n",
    "    # output confirms that there are (325 X 7) rows and (2500 + 3) columns\n",
    "    \n",
    "    export_to_csv(data, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec68bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_arr = load_dataset('eegdata.mat')\n",
    "    print_affirm('Dataset loaded')\n",
    "    \n",
    "    # set preliminary information for mne\n",
    "    n_channels = 7\n",
    "    sampling_freq = 250 \n",
    "    ch_names = ['C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'EOG']\n",
    "    ch_types = ['eeg'] * 6\n",
    "    ch_types.append('eog')\n",
    "    info = create_mne_info(n_channels=7, s_freq=sampling_freq, ch_names=ch_names, ch_types=ch_types)\n",
    "    print_affirm('MNE Info created')\n",
    "    \n",
    "    # average eye blink data\n",
    "    filtered_data_arr = mean_eye_blinks(data_arr)\n",
    "    print_affirm('Eye blinks removed from EEG data')\n",
    "\n",
    "    # find burg's coefficients after segmenting data : which will serve as the inputs\n",
    "    burgs_coeffs = segment_eeg_data(filtered_data_arr, info, 'segment')\n",
    "    print_affirm('AR coefficients obtained by Burgs method')\n",
    "    \n",
    "    # save the data to a mat file\n",
    "    save_data_to_mat(filtered_data_arr, 'mean_eye_blinks.mat')\n",
    "    print_affirm('Filtered .mat file created')\n",
    "    \n",
    "    # create a csv from the .mat file (can be used if required)\n",
    "    create_csv_dataset('mean_eye_blinks.mat', 'mean_eye_blinks')\n",
    "    print_affirm('.csv file of corrected data created and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
