{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7af80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import math\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.regression.linear_model import burg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028021ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_affirm(to_print):\n",
    "    \"\"\"\n",
    "    Helper method for pretty printing important statements \n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    to_print : any\n",
    "        the message to print\n",
    "    \"\"\"\n",
    "    print('\\u279C ' + str(to_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mne_info(s_freq, ch_names, ch_types):\n",
    "    \"\"\"\n",
    "    Method to create a global mne info object for the loaded eeg data\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    s_freq\n",
    "        The sampling frequency used for data collection\n",
    "    ch_names : array\n",
    "        An array of strings containing channel names\n",
    "    ch_types : array\n",
    "        An array of strings containing the channel types for each channel in ch_names\n",
    "    \"\"\"\n",
    "    \n",
    "    info = mne.create_info(ch_names, ch_types=ch_types, sfreq=s_freq)\n",
    "    info.set_montage('standard_1020')\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_file_name):\n",
    "    \"\"\"\n",
    "    Method to load the eeg dataset\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file_name : string\n",
    "        The name of the file containing eeg data\n",
    "    \"\"\"\n",
    "    \n",
    "    mat = scipy.io.loadmat(input_file_name)\n",
    "    return mat['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed44490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_autoregression_coeff(data_arr, annotations, index_of_zero):\n",
    "    \"\"\"\n",
    "    Method to generate Burg's coefficients \n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_arr : Numpy array\n",
    "        The data array from which to extract sub arrays\n",
    "    annotations : array\n",
    "        Array containing annotations of the eeg signals\n",
    "    annt_raw : Raw object\n",
    "        A Raw array object from MNE containing the annotated eeg data \n",
    "    \"\"\"\n",
    "    \n",
    "    burgs_values = []\n",
    "    mne.set_log_level('error')\n",
    "    for annt in annotations:\n",
    "        start = math.floor(annt['onset']*250)\n",
    "        duration = math.floor(annt['duration']*250)\n",
    "        end = start + duration\n",
    "        obs_values = []\n",
    "        for segment in data_arr[:6]:\n",
    "            # for each segment find the burg's coefficients\n",
    "            rho_sigma = list(burg(segment[start:end], order=6))\n",
    "            obs_values.append(rho_sigma)\n",
    "        burgs_values.append(obs_values)\n",
    "    print_affirm('AR coefficients obtained by Burgs method')\n",
    "    return burgs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_eeg_data(data_arr, orig_arr, info, description):\n",
    "    \"\"\"\n",
    "    Method to segment eeg data\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_arr : Numpy array\n",
    "        The data array from containing eeg data\n",
    "    info : info object from mne containing meta data\n",
    "        Info object from mne containing meta data\n",
    "    description : str\n",
    "        The description to give to each segment\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dict = {\n",
    "        'subject': [],\n",
    "        'trial': [],\n",
    "        'task': []\n",
    "    }\n",
    "    \n",
    "    coeff_arr = []\n",
    "    mne.set_log_level('error')\n",
    "    for idx, obs in enumerate(data_arr):\n",
    "        # identify the index containing the padded zeros\n",
    "        try:\n",
    "            index_of_zero = obs[0][3][0].tolist().index(0.0, 2000)\n",
    "        except ValueError:\n",
    "            index_of_zero = len(obs[0][3][0])\n",
    "        # extract the non-zero part of the segment\n",
    "        range_end = (10/2500)*len(obs[0][3][0][:index_of_zero])\n",
    "        onset_arr = np.arange(0, range_end, 0.25)\n",
    "        duration_arr = [0.5] * len(onset_arr)\n",
    "        description_arr =  [description] * len(onset_arr)\n",
    "        my_annot = mne.Annotations(\n",
    "                    onset=onset_arr, \n",
    "                    duration=duration_arr, \n",
    "                    description=description_arr\n",
    "                )\n",
    "        raw = mne.io.RawArray(obs[0][3], info)\n",
    "        raw.set_annotations(my_annot)\n",
    "        burgs_values = gen_autoregression_coeff(obs[0][3], raw.annotations, index_of_zero)\n",
    "        for burg in burgs_values:\n",
    "            for coeff_arr in burg:\n",
    "                obs_index = 1\n",
    "                data_dict['subject'].append(orig_arr[idx][0][0][-1])\n",
    "                data_dict['task'].append(orig_arr[idx][0][1][0])\n",
    "                data_dict['trial'].append((orig_arr[idx][0][2][0][-2] + orig_arr[idx][0][2][0][-1]).strip())\n",
    "                for coeff in coeff_arr[0]:\n",
    "                    if obs_index in data_dict.keys():\n",
    "                        data_dict[obs_index].append(coeff)\n",
    "                    else:\n",
    "                        data_dict[obs_index] = [coeff]\n",
    "                    obs_index += 1\n",
    "        coeff_arr.append(burgs_values)\n",
    "    print_affirm('EEG segmentation done successfully')\n",
    "    export_to_csv(data_dict, 'coefficients')\n",
    "    print_affirm('.csv file for coefficients created and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_zeros(eeg, len_of_signal):\n",
    "    \"\"\"\n",
    "    Method to remove eye blinks from the given eeg data\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    eeg : Numpy array\n",
    "        The list containing eeg signal data points with eye blinks removed \n",
    "    len_of_signal : int\n",
    "        The length of the array being passed\n",
    "    \"\"\"\n",
    "    if len_of_signal == 2500:\n",
    "        return eeg\n",
    "    zero_list = [0] * (2500 - len_of_signal)\n",
    "    padded_eeg = eeg + zero_list\n",
    "    return padded_eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_eye_blinks(data_arr): \n",
    "    \"\"\"\n",
    "    Method to remove eye blinks from the given eeg data\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_arr : Numpy array\n",
    "        The data array from which to remove eye blink data\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = data_arr\n",
    "    temp_arr = []\n",
    "    for ind, obs in enumerate(temp):\n",
    "        eog = obs[0][3][6]\n",
    "        # find peaks in the eog signal that correspond to eye blinks\n",
    "        b_peaks = signal.find_peaks(eog, height=28)\n",
    "        # find peak widths\n",
    "        b_widths = signal.peak_widths(eog, b_peaks[0], rel_height=0.6, prominence_data=None, wlen=None)\n",
    "        # obtain the left data points for the peak\n",
    "        left_points = b_widths[2]\n",
    "        # obtain the right data points for the peak\n",
    "        right_points = b_widths[3]    \n",
    "        if len(left_points) > 0 and len(right_points) > 0:\n",
    "            for eeg in obs[0][3][:6]:\n",
    "                y = eeg.tolist()\n",
    "                for idx in range(0, len(left_points)):\n",
    "                    width_arr = np.arange(int(left_points[idx]), int(right_points[idx]+1), 1)\n",
    "                    # for each data point in the width of the peak, remove that value since it corresponds to eye blink data\n",
    "                    for point in width_arr:\n",
    "                        if point < len(y):\n",
    "                            y.remove(y[point])\n",
    "                # fill the rest of the empty space of the shortened array with zero\n",
    "                padded_eeg = fill_with_zeros(y, len(y))\n",
    "                temp[ind][0][3][0] = padded_eeg\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50afb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(data, output_file_name):\n",
    "    \"\"\"\n",
    "    Method to export data to csv format\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Dictionary containing key-value pairs, with the values being equal length arrays\n",
    "    output_file_name : str\n",
    "        The file name of the output file\n",
    "    \"\"\"\n",
    "    # create a dataframe\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    # convert the dataframe to csv\n",
    "    dataframe.to_csv(output_file_name + '.csv', encoding='utf-8')\n",
    "    print(output_file_name + '.csv created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15774a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_dataset(input_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    Method to create the csv dataset from the original .mat file\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file_name : str\n",
    "        The input file name to be read\n",
    "    output_file_name : str\n",
    "        The file name of the output file\n",
    "    \"\"\"\n",
    "    \n",
    "    mat = scipy.io.loadmat(input_file_name)\n",
    "    data = mat['data'][0]\n",
    "    \n",
    "    subject_data = {\n",
    "        'subject 1': [],\n",
    "        'subject 2': [],\n",
    "        'subject 3': [],\n",
    "        'subject 4': [],\n",
    "        'subject 5': [],\n",
    "        'subject 6': [],\n",
    "        'subject 7': [],\n",
    "    }\n",
    "    # create subject wise data lists\n",
    "    for obs in data:\n",
    "        if obs[0][0][0] in subject_data.keys():\n",
    "            subject_data[obs[0][0][0]].append(obs)\n",
    "        else:\n",
    "            subject_data[obs[0][0][0]] = [obs]\n",
    "        \n",
    "    # index [0][0][3] of every subject  in subject_data contains 7 lists\n",
    "    # index [0][0][3][0] of every subject in subject_data contains 2500 values\n",
    "    for key in subject_data.keys():\n",
    "        print(key + ' has ' + str(len(subject_data[key])) + ' observations')\n",
    "        \n",
    "    # initialise the data dictionary with required keys\n",
    "    data = {\n",
    "        'subject' : [],\n",
    "        'task' : [],\n",
    "        'trial' : []\n",
    "    }\n",
    "    # for every subject, create the data row\n",
    "    for sub in subject_data.keys():\n",
    "        for obs in subject_data[sub]:\n",
    "            # loop through the 7 channels\n",
    "            for signal_values in obs[0][3]:\n",
    "                data['subject'].append(sub[-1])\n",
    "                data['task'].append(obs[0][1][0])\n",
    "                data['trial'].append((obs[0][2][0][-2] + obs[0][2][0][-1]).strip())\n",
    "                # initialise the index for the 2500 observations\n",
    "                obs_index = 1\n",
    "                # run through the 2500 values for every channel\n",
    "                for val in signal_values:\n",
    "                    if obs_index in data.keys():\n",
    "                        data[obs_index].append(val)\n",
    "                    else:\n",
    "                        data[obs_index] = [val]\n",
    "                    obs_index += 1\n",
    "    \n",
    "    # print the number of rows in the dataset \n",
    "    print('The dataset is ' + str(len(data['subject'])) + ' rows X ' + str(len(data.keys())) + ' columns')\n",
    "    # output confirms that there are (325 X 7) rows and (2500 + 3) columns\n",
    "    \n",
    "    export_to_csv(data, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d16dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_arr = load_dataset('eegdata.mat')\n",
    "    print_affirm('Dataset loaded')\n",
    "    \n",
    "    # set preliminary information for mne\n",
    "    sampling_freq = 250 \n",
    "    ch_names = ['C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'EOG']\n",
    "    ch_types = ['eeg'] * 6\n",
    "    ch_types.append('eog')\n",
    "    info = create_mne_info(s_freq=sampling_freq, ch_names=ch_names, ch_types=ch_types)\n",
    "    print_affirm('MNE Info created')\n",
    "\n",
    "    # remove eye blinks to filter data\n",
    "    filtered_data_arr = remove_eye_blinks(data_arr)\n",
    "    print_affirm('Eye blinks removed from EEG data')\n",
    "    \n",
    "    # find burg's coefficients after segmenting data : which will serve as the inputs\n",
    "    segment_eeg_data(filtered_data_arr, data_arr, info, 'segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4624cc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
